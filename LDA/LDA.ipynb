{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from cache /var/folders/00/m1hgx1hs1nj1s2tj6l7xfhd80000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.703 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.suggest_freq('沙瑞金',True)\n",
    "jieba.suggest_freq('易学习',True)\n",
    "jieba.suggest_freq('王大路',True)\n",
    "jieba.suggest_freq('京州',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个文档分词\n",
    "\n",
    "with open('nlp_test0.txt') as f:\n",
    "    text = f.read()\n",
    "    text_cut = jieba.cut(text)\n",
    "    with open('nlp_text1.txt', 'w') as fw:\n",
    "        fw.write(' '.join(text_cut))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二个文档分词\n",
    "\n",
    "with open('nlp_test2.txt') as f:\n",
    "    text = f.read()\n",
    "    text_cut = jieba.cut(text)\n",
    "    with open('nlp_text3.txt', 'w') as fw:\n",
    "        fw.write(' '.join(text_cut))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三个文档分词\n",
    "\n",
    "with open('nlp_test4.txt') as f:\n",
    "    text = f.read()\n",
    "    text_cut = jieba.cut(text)\n",
    "    with open('nlp_text5.txt', 'w') as fw:\n",
    "        fw.write(' '.join(text_cut))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件导入停用词\n",
    "\n",
    "with open('stop_words.txt', 'rb') as f:\n",
    "    stopwords_list= f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将分词后的文本重新读入\n",
    "with open('nlp_text1.txt') as f:\n",
    "    res1 = f.read()\n",
    "\n",
    "with open('nlp_text3.txt') as f:\n",
    "    res2 = f.read()\n",
    "with open('nlp_text5.txt') as f:\n",
    "    res3 = f.read()\n",
    "\n",
    "corpus = [res1, res2, res3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 43)\t1\n  (0, 74)\t1\n  (0, 19)\t1\n  (0, 56)\t1\n  (0, 36)\t1\n  (0, 97)\t1\n  (0, 76)\t1\n  (0, 31)\t1\n  (0, 66)\t1\n  (0, 47)\t1\n  (0, 12)\t1\n  (0, 81)\t1\n  (0, 3)\t2\n  (0, 44)\t2\n  (0, 83)\t2\n  (0, 86)\t1\n  (0, 92)\t1\n  (0, 8)\t2\n  (0, 70)\t5\n  (0, 26)\t1\n  (0, 25)\t1\n  (0, 94)\t1\n  (0, 96)\t1\n  (0, 85)\t1\n  (0, 33)\t1\n  :\t:\n  (2, 82)\t1\n  (2, 59)\t1\n  (2, 45)\t1\n  (2, 51)\t1\n  (2, 29)\t1\n  (2, 52)\t2\n  (2, 73)\t1\n  (2, 62)\t1\n  (2, 17)\t1\n  (2, 89)\t1\n  (2, 75)\t1\n  (2, 41)\t2\n  (2, 32)\t1\n  (2, 27)\t1\n  (2, 71)\t1\n  (2, 11)\t1\n  (2, 48)\t1\n  (2, 34)\t1\n  (2, 49)\t2\n  (2, 69)\t1\n  (2, 7)\t1\n  (2, 6)\t1\n  (2, 64)\t1\n  (2, 0)\t1\n  (2, 81)\t1\n"
     ]
    }
   ],
   "source": [
    "# 建立词频向量，注意LDA是基于词频，不是TD-IDF值\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "cnVector = CountVectorizer(stop_words=stopwords_list)\n",
    "cntTf = cnVector.fit_transform(corpus)\n",
    "print(cntTf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.008541    0.991459  ]\n [ 0.01356865  0.98643135]\n [ 0.98516434  0.01483566]]\n[[ 1.46629538  1.45111821  0.58439537  0.55155707  0.5587759   0.5758372\n   1.45507943  1.44802816  0.56802888  0.57133116  0.58552689  1.44468477\n   0.5297546   0.57356316  0.59748027  0.54119482  0.60120747  1.46265254\n   0.56459639  0.53633011  0.58254482  1.45977402  0.54466221  0.563354\n   0.55142177  0.55179511  0.53850455  1.44457928  0.56180357  1.45074334\n   0.56713191  0.56086194  1.44060454  0.55266177  1.43864567  0.53817287\n   0.54002689  0.56519004  0.57594902  0.56817657  0.57286625  2.33633043\n   0.56222347  0.55621121  0.54714321  1.45651418  0.57225027  0.53989587\n   1.45976442  2.34116601  0.5523016   1.4599907   2.34767437  1.45424602\n   0.5859677   0.57245829  0.54197964  0.59730467  0.55148062  1.4391995\n   0.61016888  0.58945744  1.44415007  0.60368422  1.45347659  0.58512495\n   0.56091701  0.55568911  0.56788225  1.46133515  0.61025662  1.46704894\n   0.54827913  1.45239911  0.54950252  1.45408444  0.56175678  1.43608666\n   1.44836391  0.57579752  0.54662435  1.48044913  1.44614053  0.59955583\n   0.55522326  0.54920509  0.5439877   0.55215031  0.56390247  1.46814236\n   0.55464439  0.56513284  0.54522184  1.43174397  0.56828096  0.5594873\n   0.54473118  0.56425709]\n [ 0.5441596   0.5454515   2.32912318  2.32208201  2.34346934  1.41220025\n   0.54133309  0.55307397  2.32067228  1.43419819  1.43531358  0.55079913\n   1.46937428  1.44689564  1.42314663  1.44151136  2.32374196  0.55931419\n   3.22054088  1.43664646  1.43278486  0.55135181  1.45389491  2.32283071\n   1.43339709  1.43896037  1.47008288  0.55214234  2.32138898  0.55759479\n   1.43350688  1.44529837  0.53845632  1.43801748  0.54711158  1.44878751\n   1.43210477  2.34459272  1.43544167  1.43955418  1.42927049  0.55056127\n   1.44619378  1.45513787  2.34357642  0.54652275  1.43915303  1.45176424\n   0.5367367   0.5601429   1.43695772  0.55182188  0.53421019  0.55442136\n   2.30894969  1.43202964  1.46623447  5.00609231  1.44879801  0.55146946\n   3.18496462  2.33949219  0.54552389  3.21551294  0.54192417  3.23172755\n   1.43600527  1.42793315  1.45996719  0.54169819  7.66779625  0.55847909\n   1.4393201   0.55637391  1.44223603  0.54550807  1.42860683  0.53926468\n   0.55993721  1.42853417  1.44226541  2.33706384  0.54858861  3.21418627\n   1.44245873  1.44237303  1.44686128  1.4338959   1.41900184  0.54906497\n   1.42558544  1.453039    1.4523151   0.5409102   1.42381228  1.44781373\n   1.45569234  1.43762561]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinru/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n  DeprecationWarning)\n/Users/xinru/anaconda2/envs/python3/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=2, learning_offset=5, random_state=0)\n",
    "docres = lda.fit_transform(cntTf)\n",
    "print(docres)\n",
    "print(lda.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
